{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `This notebook reads the audios (in \"s##e##.wav\") in folder \"audios\" and extracts the text info from the audio. The get_time_nodes() function reads in rttm file and split the audio by the time when speaker role changes. The output data of the audio script will be stored into folder \"timeData\". The extract_whole_text() does the speech recognition on the whole input video. Note that wav2vec cannot process long audios. Create following folders before running the file:` \n",
    "\n",
    "- audios: all audio files for WILTY\n",
    "- rttm: find all rttm files\n",
    "- timeData: the folder where output files are stored\n",
    "\n",
    "Last updated on August 3rd, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'librosa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/yuki/Desktop/newfolder/preprocessing/diartSegGenerator.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yuki/Desktop/newfolder/preprocessing/diartSegGenerator.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlibrosa\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yuki/Desktop/newfolder/preprocessing/diartSegGenerator.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yuki/Desktop/newfolder/preprocessing/diartSegGenerator.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'librosa'"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "import moviepy.editor as mp\n",
    "\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC\n",
    "from datasets import load_dataset\n",
    "import librosa\n",
    "import datasets\n",
    "import torch\n",
    "from IPython.display import Audio\n",
    "import moviepy.editor as mp\n",
    "import os\n",
    "import pandas as pd\n",
    "import sox\n",
    "\n",
    "# import model, feature extractor, tokenizer\n",
    "model = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\",sampling_rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_nodes(filename):\n",
    "    '''\n",
    "    parameter: \n",
    "    filename: str. the name of the audio without the suffix\n",
    "\n",
    "    reads in the rttm file in folder \"rttm\"\n",
    "\n",
    "    returns:\n",
    "    timeNodes: list., time spot that speaker changes\n",
    "    speakerID: list, same length with the timeNodes, marks the corrosponding speaker\n",
    "    '''\n",
    "    f = open(\"..rttm/\"+filename+\".rttm\", \"r\")\n",
    "    data=f.read().split('\\n')[:-1]\n",
    "\n",
    "    #time and speaker data from the file\n",
    "    onset=[float(d.split(\" \")[3]) for d in data]\n",
    "    duration=[float(d.split(\" \")[4]) for d in data]\n",
    "    speaker=[int(d.split(\" \")[7][-1]) for d in data]\n",
    "\n",
    "    #initialize variables\n",
    "    current=0\n",
    "    speakerID=[]\n",
    "    timeNodes=[]\n",
    "\n",
    "    for i in range(len(speaker)):\n",
    "        if speaker[i]!=current:\n",
    "            timeNodes.append(onset[i])\n",
    "            speakerID.append(speaker[i-1])\n",
    "            current=speaker[i]\n",
    "\n",
    "    return timeNodes,speakerID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_snippet(nameOfAudio,listOfTime):\n",
    "    '''\n",
    "    parameter: \n",
    "    nameOfAudio: str. the name of audio to be processed without the suffix\n",
    "    listOfTime: an array\n",
    "\n",
    "    read in audio files from the folder audios\n",
    "    generate audio snippet, desample the audio into audio.mp3, and extract the snippet into temp.mp3\n",
    "    extract text from the temp.mp3\n",
    "\n",
    "    output stored to folder \"timeData\"\n",
    "    '''\n",
    "    \n",
    "    #desample\n",
    "    os.system('sox audios/'+nameOfAudio+'.wav -r 16000 audio.mp3')\n",
    "    dataList=[]\n",
    "\n",
    "    for i in range(len(listOfTime)-1):\n",
    "        timeOff=listOfTime[i]\n",
    "        ffmpeg_extract_subclip(\"audio.mp3\", listOfTime[i], listOfTime[i+1], targetname=\"temp.mp3\")\n",
    "\n",
    "        # Load the audio with the librosa library\n",
    "        input_audio, sr = librosa.load(\"temp.mp3\", sr=16000)\n",
    "\n",
    "        # forward sample through model to get greedily predicted transcription ids\n",
    "        input_values = feature_extractor(input_audio, return_tensors=\"pt\").input_values\n",
    "        logits = model(input_values).logits[0]\n",
    "        pred_ids = torch.argmax(logits, axis=-1)\n",
    "\n",
    "        # retrieve word stamps (analogous commands for `output_char_offsets`)\n",
    "        outputs = tokenizer.decode(pred_ids, output_word_offsets=True)\n",
    "        # compute `time_offset` in seconds as product of downsampling ratio and sampling_rate\n",
    "        time_offset = model.config.inputs_to_logits_ratio / feature_extractor.sampling_rate\n",
    "    \n",
    "        for d in outputs.word_offsets:\n",
    "            dataList.append([d[\"word\"],round(d[\"start_offset\"] * time_offset+timeOff, 2),round(d[\"end_offset\"] * time_offset+timeOff, 2)])\n",
    "    df=pd.DataFrame(dataList, columns=('word', 'start_time', 'end_time'))\n",
    "    df.to_csv(\"timeData/\"+nameOfAudio+\".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_whole_text(filename):\n",
    "    '''\n",
    "    parameter:\n",
    "    filename: the name of the audio without the suffix\n",
    "\n",
    "    does the text extraction based on the timeSeg split\n",
    "    '''\n",
    "    timeNodes,_=get_time_nodes(filename)\n",
    "    timeNodes.insert(0,0)\n",
    "    extract_text_snippet(filename,timeNodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_whole_text(\"utility/videoAnnotate/s01e03\")\n",
    "# print(get_time_nodes(\"s01e03\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e4e1b29a8f3351e0554dd3a0af00c8fcb68193190a462336a502cdea2c36c18"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ve')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
