{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `The final output of this file is a cleared-up dataset called \"dataset.npy\" stored in the current directory.`\n",
    "##### `Folder Usage:`\n",
    "- Glove, normalization, opensmile, praat: tool.\n",
    "- helper_files: five csv/txt file for lexical analysis and gender, possession classification.\n",
    "- guestInfo: guest names for each episode, 14 files, one per season. \n",
    "- jsonlOut: 14 jsonl files storing annotation info one per season.\n",
    "- audio: store all audio files.\n",
    "- temp_files: empty, for intermediate files generated in the process of running the code.\n",
    "\n",
    "##### `Some parameters to pass in:`\n",
    "- the directory to store the raw data (in csv) of records per episode. See the bottom of function `extract_features()`\n",
    "- which seasons to run. Add/delete name of json file (without suffix) in the parameter list of `extract_features()`. See the block below `extract_features()`.\n",
    "- which subset of the original raw dataset. Change the `subsetType` parameter in function `make_feature_vectors()`.\n",
    "\n",
    "Last updated on August 2nd, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mfcc\n",
    "import python_speech_features as psf\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "#ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy\n",
    "\n",
    "# embedding\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import spatial\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "# #other\n",
    "import pandas as pd\n",
    "import textExtrator as te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data for later uses\n",
    "\n",
    "#initialize the embed_dict for later use\n",
    "#reference: https://analyticsindiamag.com/hands-on-guide-to-word-embeddings-using-glove/\n",
    "embed_dict = {}\n",
    "with open('glove/glove.twitter.27B.200d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0][1:-1]\n",
    "        vector = np.array(values[1:],'float32')\n",
    "        embed_dict[word]=vector\n",
    "\n",
    "#reads in the concreteness rating for later use\n",
    "concreteness_df = pd.read_excel(\"helper_files/concreteness.xlsx\",usecols=[0,2])\n",
    "concreteness_dict = {}\n",
    "for index in range(len(concreteness_df)):\n",
    "    concreteness_dict[concreteness_df[\"Word\"][index]]=concreteness_df[\"Conc.M\"][index]\n",
    "\n",
    "#create hedge word list\n",
    "#reference: https://github.com/words/hedges\n",
    "hedge_list = []  \n",
    "with open ('helper_files/hedge.txt','r') as f:\n",
    "    for line in f:\n",
    "        if line[0]!='%' and line[0]!='\\n':\n",
    "            hedge_list.append(line[:-1])\n",
    "\n",
    "#create weasel word list\n",
    "#https://github.com/words/weasels        \n",
    "weasels_list = []\n",
    "with open ('helper_files/weasels.txt','r') as f:\n",
    "    for line in f:\n",
    "        if line[0]!='%' and line[0]!='\\n':\n",
    "            weasels_list.append(line[:-1])\n",
    "\n",
    "#create the name-gender dictionary\n",
    "gender_df = pd.read_csv(\"helper_files/wilty_genders.csv\",usecols=[0,1])\n",
    "gender_dict = {}\n",
    "for index in range(len(gender_df)):\n",
    "    gender_dict[gender_df[\"Name\"][index]]=gender_df[\"Gender\"][index]\n",
    "\n",
    "#create the list of possession names\n",
    "possessionList = []\n",
    "possession = pd.read_csv(\"helper_files/possession.csv\")\n",
    "for line in possession.index:\n",
    "    possessionList.append(possession[\"EpisodeID\"].iloc[line]+\"-\"+str(possession[\"Index\"].iloc[line]))\n",
    "\n",
    "#create the list of guest names\n",
    "guestDict={}\n",
    "file_list = os.listdir(\"guestInfo\")\n",
    "try:\n",
    "    file_list.remove(\".DS_Store\")\n",
    "except:\n",
    "    pass\n",
    "for file in file_list:\n",
    "    guest=pd.read_csv(f\"guestInfo/{file}\")\n",
    "    for i in guest.index:\n",
    "        guestDict[guest.iloc[i][\"EpisodeID\"]]=guest.iloc[i][\"Names\"].split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process the jsonl file\n",
    "import json\n",
    "def convert_jsonl_to_list(audioName):\n",
    "    '''\n",
    "    parameter:\n",
    "    audioName: str. the name of the jsonl file without suffix\n",
    "\n",
    "    return:\n",
    "    turnList: list. in the form of [[startTime, endTime, label], [], ...]\n",
    "    '''\n",
    "    with open(f'{audioName}.jsonl', 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    for json_str in json_list:\n",
    "        jsonl = json.loads(json_str)\n",
    "\n",
    "    timeDicts = jsonl[\"audio_spans\"]\n",
    "    startTimeList = set([dictionary[\"start\"] for dictionary in timeDicts])\n",
    "    turnList=[]\n",
    "    \n",
    "    for index, start in enumerate(startTimeList):\n",
    "        for dicts in timeDicts:\n",
    "            if dicts[\"start\"]==start and dicts[\"label\"][:3]!= \"SEG\":\n",
    "                turnList.append([dicts[\"start\"],dicts[\"end\"]])\n",
    "\n",
    "    for timeList in turnList:\n",
    "        for dicts in timeDicts:\n",
    "            if timeList[0] == dicts[\"start\"] and dicts[\"label\"][:3]==\"SEG\":\n",
    "                timeList.append(dicts[\"label\"][-5:])\n",
    "\n",
    "    return turnList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acoustic features\n",
    "def extract_opensmile():\n",
    "    '''\n",
    "    extract the opensmile info\n",
    "    '''\n",
    "    os.system(\"opensmile/bin/SMILExtract -C opensmile/config/is09-13/IS13_ComParE.conf -I temp_files/temp.wav -O temp_files/temp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lexical features\n",
    "def extract_ngram(text, trigramDic, ngram_size=3):\n",
    "    '''\n",
    "    parameter:\n",
    "    text: str. the string to be processed\n",
    "    trigramDic: the dictionary to be passed in\n",
    "    ngram_size: int. number of grams\n",
    "\n",
    "    reference:\n",
    "    https://tousu.in/qa/?qa=1008060/python-how-to-use-sklearns-countvectorizerand-to-get-ngrams-that-include-any-punctuation-as-separate-tokens\n",
    "    '''\n",
    "    vect = CountVectorizer(analyzer='word',ngram_range=(ngram_size,ngram_size))\n",
    "    X = vect.fit_transform([text])\n",
    "    trigramArr= vect.get_feature_names_out()\n",
    "    frequencyArr = X.toarray()[0]\n",
    "    for i in range(trigramArr.shape[0]):\n",
    "        try:\n",
    "            trigramDic[trigramArr[i]] += frequencyArr[i]\n",
    "        except:\n",
    "            trigramDic[trigramArr[i]] = frequencyArr[i]\n",
    "  \n",
    "            \n",
    "def extract_lexical_features(text, dataframe, rowNum):\n",
    "    '''\n",
    "    parameter:\n",
    "    text: str. the string to be processed from one speech turn\n",
    "    dataframe: pdDataframe. the name of the dataframe where info should store\n",
    "    rowNum: int. the row index where the record should be filled in\n",
    "\n",
    "    reference: https://spacy.io/usage/linguistic-features#sbd\n",
    "    ''' \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    #allTokenList is the pure text after normalization\n",
    "    allTokenList = [token.text for token in doc]\n",
    "    posList = [token.pos_ for token in doc]\n",
    "    #tokenList has every term in lemma form and decapitalized\n",
    "    tokenList = []\n",
    "    for token in doc:\n",
    "        if token.pos_!=\"PUNCT\":\n",
    "            #here I make every word lower case and take the lemma form\n",
    "            tokenList.append(token.lemma_.lower())\n",
    "    print(tokenList)\n",
    "\n",
    "    #get the embedding\n",
    "    tokenArr=[]\n",
    "    for token in allTokenList:\n",
    "        try:\n",
    "            vec=embed_dict[token.lower()]\n",
    "            tokenArr.append(vec)\n",
    "        except:\n",
    "            pass\n",
    "    embedding=np.average(np.array(tokenArr), axis=0).tolist()\n",
    "\n",
    "    #count number of words in each type\n",
    "    numVerb=posList.count(\"VERB\")\n",
    "    numNoun=posList.count(\"NOUN\")\n",
    "    numAdj=posList.count(\"ADJ\")\n",
    "    numNum=posList.count(\"NUM\")\n",
    "\n",
    "    #calculate word and sentence level parameters\n",
    "    numSent=len([sent for sent in doc.sents])\n",
    "    numWords=len(tokenList)\n",
    "    wordsPerSent=numWords/numSent\n",
    "    typeTokenRatio=len(set(tokenList))/len(tokenList)\n",
    "    wordg6=0\n",
    "    for word in tokenList:\n",
    "        if len(word)>6:\n",
    "            wordg6+=1\n",
    "    \n",
    "    #get the concreteness for the lemma word list\n",
    "    concretenessList=[]\n",
    "    for token in tokenList:\n",
    "        try:\n",
    "            concretenessList.append(concreteness_dict[token])\n",
    "        except:\n",
    "            pass\n",
    "    try:\n",
    "        concreteness = int(np.array(concretenessList).mean())\n",
    "    except:\n",
    "        #not sure if could do this\n",
    "        concreteness = 0\n",
    "        \n",
    "    #hedge words & weasel words\n",
    "    hedgeList=[]\n",
    "    weaselsList=[]\n",
    "    #if the same word appears multiple times, count them multiple times\n",
    "    for token in tokenList:\n",
    "        if token in hedge_list:\n",
    "            hedgeList.append(token)\n",
    "        if token in weasels_list:\n",
    "            weaselsList.append(token)\n",
    "    numHedge = len(hedgeList)\n",
    "    numWeasel = len(weaselsList)\n",
    "    ifHedge = 0 if numHedge == 0 else 1\n",
    "    ifWeasel = 0 if numWeasel == 0 else 1\n",
    "\n",
    "    #get the filled pauses\n",
    "    numFP, ifFP = te.get_filled_pauses()\n",
    "    \n",
    "    dataframe.loc[rowNum, [\"tokenization\", \"posTag\", \"lemmaForm\", \"avgEmbedding\",\"hasFilledPause\", \"#filledPause\",\\\n",
    "        \"hasHedgeWord\", \"#hedgeWord\", \"hasWeaselWord\", \"#weaselWord\", \"#sent\", \"#word\", \"#word/sent\", \"#len(word)>6\",\\\n",
    "        \"typeTokenRatio\", \"#verb\", \"#noun\", \"#adj\", \"#num\", \"concreteness\"]] = \\\n",
    "        [allTokenList, posList, tokenList, embedding, ifFP, numFP, ifHedge, numHedge, ifWeasel,\\\n",
    "        numWeasel, numSent, numWords, wordsPerSent, wordg6, typeTokenRatio, numVerb, numNoun, numAdj, numNum,\\\n",
    "        concreteness]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(outputFileName, save2CSV = False):\n",
    "    '''\n",
    "    parameter:\n",
    "    outputFileName: str. or list. the name/list of names of the jsonl file for the whole season\n",
    "    saveToCSV: bool. whether to save the raw records to csv, default false\n",
    "\n",
    "    output:\n",
    "    store the output data to the folder indicated, currently vector_info_csv\n",
    "\n",
    "    return:\n",
    "    allFeatures: npArr. the list of all features for all records\n",
    "    name: npArr. the speaker name for each of the record\n",
    "    statementIDList: npArr. the statementID for each of the record in the form of sxxexx-x, where x is a number.\n",
    "    textList: npArr. the big list consisting of all texts for each record\n",
    "\n",
    "    NOTE: make sure indicated folders do exist!\n",
    "    '''\n",
    "    textList = []\n",
    "    #preprocess the json files if the input is a list\n",
    "    if type(outputFileName) is list:\n",
    "        with open(\"jsonlOut/all.jsonl\",\"w\") as outfile:\n",
    "            for name in outputFileName:\n",
    "                with open(f'jsonlOut/{name}.jsonl','r') as json_file:\n",
    "                    try:\n",
    "                        while True:\n",
    "                            jsonOb = json.loads(json_file.readline())\n",
    "                            json.dump(jsonOb, outfile)\n",
    "                            outfile.write(\"\\n\")\n",
    "                    except:\n",
    "                        json_file.close()\n",
    "                        continue\n",
    "        outfile.close()\n",
    "        outputFileName = \"all\"\n",
    "\n",
    "    #intialize a global trigram as a dictionary\n",
    "    trigram = {}\n",
    "    #initialize a global list that stores all the names and statementID\n",
    "    name = []\n",
    "    statementIDList = []\n",
    "    #intialize a global list storing all feature vectors\n",
    "    allFeatures = []\n",
    "\n",
    "    #read in the json file\n",
    "    with open(f'jsonlOut/{outputFileName}.jsonl', 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "        \n",
    "    for json_str in json_list:\n",
    "        jsonl = json.loads(json_str)\n",
    "        rawTimeDicts = jsonl[\"audio_spans\"]\n",
    "        audioName = jsonl[\"video\"][-10:-4]\n",
    "\n",
    "        #clean the rawTimeDicts\n",
    "        timeDicts = []\n",
    "        for i in range(len(rawTimeDicts)):\n",
    "            if rawTimeDicts[i][\"label\"][0]!=\"S\":\n",
    "                timeDicts.append(rawTimeDicts[i])\n",
    "\n",
    "        #sort the dictionary in the list\n",
    "        startList=[]\n",
    "        endList=[]\n",
    "        speakerList=[]\n",
    "        labelList=[]\n",
    "        speakerNameList=[]\n",
    "\n",
    "        #add values to corresponding lists\n",
    "        for dict in timeDicts:\n",
    "            if dict[\"label\"]==\"start\":\n",
    "                startList.append(dict[\"start\"])\n",
    "            elif dict[\"label\"][:3]==\"end\":\n",
    "                endList.append(dict[\"end\"])\n",
    "                if dict[\"label\"][4]==\"t\":\n",
    "                    #NOTE: true is 1 and false is 0\n",
    "                    labelList.append(1)\n",
    "                else:\n",
    "                    labelList.append(0)\n",
    "            else:\n",
    "                speakerList.append(dict[\"start\"])\n",
    "                speakerNameList.append([dict[\"start\"], dict[\"end\"], dict[\"label\"]])\n",
    "\n",
    "        startList.sort()\n",
    "        endList.sort()\n",
    "        endOrder=np.array(endList).argsort()\n",
    "        speakerOrder=np.array(speakerList).argsort()\n",
    "        labelList=np.array(labelList)[endOrder]\n",
    "        speakerNameList=np.array(speakerNameList)[speakerOrder]\n",
    "\n",
    "        # the structure of the final list: [start, end, segment_duration, speaker_name, label]\n",
    "        turnList=[[float(speakerNameList[i][0]), float(speakerNameList[i][1]), float(endList[i])-float(startList[i]), \\\n",
    "            guestDict[audioName][int(speakerNameList[i][2][-1])-1].strip() if len(speakerNameList[i][2])==2 \\\n",
    "            else speakerNameList[i][2][1:], labelList[i]] for i in range(len(speakerList))]\n",
    "\n",
    "        #turnlist.form = [[startTime, endTime, label], [], ...]\n",
    "\n",
    "        #create the macro dataframe that each loop adds one record of data\n",
    "        featureDataframe=pd.DataFrame(columns=[\"rawText\",\"startTime\", \"endTime\", \"utteranceDuration\", \"sectionDuration\",\\\n",
    "            \"speakerName\", \"label\", \"wordSeg\", \"tokenization\", \"posTag\", \"lemmaForm\", \"openSmile\", \"ngram\", \"avgEmbedding\",\\\n",
    "            \"hasFilledPause\", \"#filledPause\", \"hasHedgeWord\", \"#hedgeWord\", \"hasWeaselWord\", \"#weaselWord\", \"#sent\",\\\n",
    "            \"#word\", \"#word/sent\", \"#len(word)>6\", \"typeTokenRatio\", \"#verb\", \"#noun\", \"#adj\", \"#num\", \\\n",
    "            \"concreteness\"])\n",
    "    \n",
    "        for i in range(len(turnList)):\n",
    "            #exclude the possession record if 'addPossession' variable is false\n",
    "            statementID = audioName+str(i+1)\n",
    "            \n",
    "            #add the statementID to the list\n",
    "            statementIDList.append(statementID[:-1]+\"-\"+statementID[-1])\n",
    "\n",
    "            #extract acoustic features\n",
    "            te.write_temp_file(audioName,turnList[i][0],turnList[i][1])\n",
    "\n",
    "            #acoustic features\n",
    "            extract_opensmile()\n",
    "            with open(\"temp_files/temp.csv\", \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "            myList = lines[-1].split(\",\")\n",
    "            opensmile = myList[1:-1] \n",
    "    \n",
    "            #lexical features\n",
    "            seg, text = te.asr()\n",
    "            #did text[:-1] so that the new-line character is not included\n",
    "            text = text[:-1]\n",
    "            textList.append(text)\n",
    "            extract_ngram(text, trigram)\n",
    "            name.append(turnList[i][3])\n",
    "            extract_lexical_features(text, featureDataframe, i)\n",
    "\n",
    "            #write variables into the dataframe\n",
    "            featureDataframe.loc[i,[\"rawText\",\"startTime\",\"endTime\",\"utteranceDuration\",\"sectionDuration\",\"speakerName\",\"label\",\"wordSeg\",\"openSmile\", \"ngram\"]] \\\n",
    "                =[text, turnList[i][0], turnList[i][1],turnList[i][1]-turnList[i][0], turnList[i][2], turnList[i][3], int(turnList[i][4]), seg, opensmile, trigram]\n",
    "            \n",
    "            allFeatures.append(featureDataframe.loc[i, [\"utteranceDuration\", \"sectionDuration\", \"label\",\"openSmile\", \"ngram\", \"avgEmbedding\",\\\n",
    "            \"hasFilledPause\", \"#filledPause\", \"hasHedgeWord\", \"#hedgeWord\", \"hasWeaselWord\", \"#weaselWord\", \"#sent\",\\\n",
    "            \"#word\", \"#word/sent\", \"#len(word)>6\", \"typeTokenRatio\", \"#verb\", \"#noun\", \"#adj\", \"#num\", \\\n",
    "            \"concreteness\"]].tolist())\n",
    "\n",
    "            #save to csv if 'save2CSV' is true\n",
    "            if save2CSV:\n",
    "                featureDataframe.to_csv(f\"vector_info_csv/{audioName}.csv\")\n",
    "                \n",
    "    return np.array(allFeatures), np.array(name), np.array(statementIDList), np.array(textList)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Change file input in the block below.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFeatures, name, id, txtList = extract_features([\"s01out\", \"s02out\", \"s03out\", \"s04out\", \"s05out\", \"s06out\", \"s07out\", \"s08out\", \"s09out\",\"s10out\", \"s11out\", \"s12out\", \"s13out\", \"s14out\"], save2CSV=True)\n",
    "# allFeatures, name, id, txtList = extract_features([\"s06out\"], save2CSV=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the index of multiple things\n",
    "#1. all; 2. possession only 3. three hosts only 4. other than possession and hosts\n",
    "def clear_subset(featureArr, nameArr, statementIDArr, textArr, subsetType):\n",
    "    '''\n",
    "    featureArr: list. the list of ALL features from the audio\n",
    "    nameArr: list. the list of speakers that have one-to-one correspondce to each of ALL feature vector\n",
    "    statementIDArr: list. the list of uniqiue statement IDs that have one-to-one correspondence to each of ALL feature vector\n",
    "    textArr: list. the list of the ALL the texts\n",
    "    subsetType: str. indicate the type of subset. Choose from \"all\", \"posessionOnly\", \"hostOnly\", \"possession&Host\", or \"regularOnly\"\n",
    "\n",
    "    return: \n",
    "    feature: npArr. the array of ALL features from the audio\n",
    "    name: npArr. the array of speakers that have one-to-one correspondce to each of ALL feature vector\n",
    "    statementID: npArr. the array of statement ids for the subset\n",
    "    ngramTemplate: npArr. the list of all high freq ngrams\n",
    "    ngramArr: npArr. the list of all grams for each record\n",
    "    '''\n",
    "\n",
    "    #possession only index\n",
    "    possessionIndex = []\n",
    "    for i in range(id.shape[0]):\n",
    "        if id[i] in possessionList:\n",
    "            possessionIndex.append(i)\n",
    "\n",
    "    #speaker only index\n",
    "    nameDf = pd.DataFrame(nameArr)\n",
    "    nameDf.value_counts().to_csv(\"speakerFreq.csv\")\n",
    "    removalNameList = [\"david\", \"lee\", \"rob\", \"angus\"]\n",
    "    hostIndex = []\n",
    "    for i in range(nameArr.shape[0]):\n",
    "        if nameArr[i] in removalNameList:\n",
    "            hostIndex.append(i)\n",
    "\n",
    "    #all aside\n",
    "    allAside = list(set(possessionIndex+hostIndex))\n",
    "\n",
    "    #all need\n",
    "    allNeed = []\n",
    "    for i in range(featureArr.shape[0]):\n",
    "        if i not in allAside:\n",
    "            allNeed.append(i)\n",
    "\n",
    "    if subsetType == \"all\":\n",
    "        index = list(range(len(featureArr.shape[0])))\n",
    "    elif subsetType == \"possessionOnly\":\n",
    "        index = possessionIndex\n",
    "    elif subsetType == \"hostOnly\":\n",
    "        index = hostIndex\n",
    "    elif subsetType == \"possession&Host\":\n",
    "        index = allAside\n",
    "    else:\n",
    "        index = allNeed\n",
    "    \n",
    "    #make smaller datasets\n",
    "    feature = featureArr[index]\n",
    "    name = nameArr[index]\n",
    "    text = textArr[index]\n",
    "    statementID = statementIDArr[index]\n",
    "\n",
    "    #make ngram template\n",
    "    corpus = text.tolist()\n",
    "    nlp=spacy.load(\"en_core_web_sm\")\n",
    "    tokens = {}\n",
    "    for i in range(1,len(corpus)+1):\n",
    "        doc = nlp(corpus[i-1])\n",
    "        #x.text is the string form\n",
    "        subtokens = [x.text.lower() for x in doc]\n",
    "        #why making tokens into a dictionary?\n",
    "        tokens[i] = subtokens\n",
    "    \n",
    "    vect=CountVectorizer(input=\"content\", lowercase=True, preprocessor= lambda x:x, tokenizer=lambda key:tokens[key],ngram_range=(1,1))\n",
    "    X = vect.fit_transform(tokens.keys())\n",
    "    ngramFreq = X.toarray().sum(axis=0).astype(int)\n",
    "    ngramIndex = np.argwhere(ngramFreq>=5).flatten()\n",
    "    ngramTemplate = vect.get_feature_names_out()[ngramIndex].tolist()\n",
    "    vect=CountVectorizer(input=\"content\", lowercase=True, preprocessor= lambda x:x, tokenizer=lambda key:tokens[key],ngram_range=(2,2))\n",
    "    X = vect.fit_transform(tokens.keys())\n",
    "    ngramFreq = X.toarray().sum(axis=0).astype(int)\n",
    "    ngramIndex = np.argwhere(ngramFreq>=5).flatten()\n",
    "    ngramTemplate = ngramTemplate + vect.get_feature_names_out()[ngramIndex].tolist()\n",
    "    vect=CountVectorizer(input=\"content\", lowercase=True, preprocessor= lambda x:x, tokenizer=lambda key:tokens[key],ngram_range=(3,3))\n",
    "    X = vect.fit_transform(tokens.keys())\n",
    "    ngramFreq = X.toarray().sum(axis=0).astype(int)\n",
    "    ngramIndex = np.argwhere(ngramFreq>=5).flatten()\n",
    "    ngramTemplate = ngramTemplate + vect.get_feature_names_out()[ngramIndex].tolist()\n",
    "\n",
    "    #create ngram for each record\n",
    "    ngramList = []\n",
    "    for c in corpus:\n",
    "        tokens = {}\n",
    "        doc = nlp(c)\n",
    "        #x.text is the string form\n",
    "        subtokens = [x.text.lower() for x in doc]\n",
    "        #why making tokens into a dictionary?\n",
    "        tokens[i] = subtokens\n",
    "    \n",
    "        vect=CountVectorizer(input=\"content\", lowercase=True, preprocessor= lambda x:x, tokenizer=lambda key:tokens[key],ngram_range=(1,1))\n",
    "        X = vect.fit_transform(tokens.keys())\n",
    "        ngram = vect.get_feature_names_out().tolist()\n",
    "        vect=CountVectorizer(input=\"content\", lowercase=True, preprocessor= lambda x:x, tokenizer=lambda key:tokens[key],ngram_range=(2,2))\n",
    "        X = vect.fit_transform(tokens.keys())\n",
    "        ngram = ngram + vect.get_feature_names_out().tolist()\n",
    "        vect=CountVectorizer(input=\"content\", lowercase=True, preprocessor= lambda x:x, tokenizer=lambda key:tokens[key],ngram_range=(3,3))\n",
    "        X = vect.fit_transform(tokens.keys())\n",
    "        ngram = ngram + vect.get_feature_names_out().tolist()\n",
    "        ngramList.append(ngram)\n",
    "\n",
    "    print(\"possesion:\", len(possessionIndex), \" hosts:\", len(hostIndex), \" allAside:\", len(allAside), \" allNeed:\", len(allNeed))\n",
    "    return feature, name, statementID, np.array(ngramTemplate), np.array(ngramList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_vectors(featureArr, nameArr, statementIDArr, textArr, subsetType=\"regularOnly\"):\n",
    "    '''\n",
    "    paratemers:\n",
    "    featureArr: npArr. the numpy array of ALL features from the audio\n",
    "    nameList: npArr. the numpy array of speakers that have one-to-one correspondce to each of ALL feature vector\n",
    "    statementIDList: npArr. the numpy array of uniqiue statement IDs that have one-to-one correspondence to each of ALL feature vector\n",
    "    textList: npArr. the numpy array of the ALL texts\n",
    "    subsetType: str. indicate the type of subset. Choose from \"all\", \"posessionOnly\", \"hostOnly\", \"possession&Host\", or \"regularOnly\"\n",
    "\n",
    "    output:\n",
    "    featureVectorList: npArr. the numpy array of feature vectors that could be put into training model.\n",
    "                    The last column is the dependent variable, i.e truth/lie\n",
    "    dataset.npy: file. the final dataset stored in the current directory\n",
    "    '''\n",
    "\n",
    "    #feature.form = [utteranceDuration, sectionDuration, label, openSmile(6373), ngram, avgEmbedding(200), hasFilledPause, #filledPause,\\\n",
    "    #    hasHedgeWord, #hedgeWord, hasWeaselWord, #weaselWord, #sent, #word, #word/sent, #len(word)>6, typeTokenRatio,\\\n",
    "    #    #verb, #noun, #adj, #num, concreteness]\n",
    "\n",
    "    #NOTE: SOME SPECIAL POSITIONS\n",
    "    #pos@2: label, i.e. dependent variable\n",
    "    #pos@3: opensmile list\n",
    "    #pos@4: ngram vec\n",
    "    \n",
    "    feature, name, statementID, ngramTemplate, ngramArr = clear_subset(featureArr, nameArr, statementIDArr, textArr, subsetType)\n",
    "    \n",
    "    processedFeatureVec = []\n",
    "    preFeatureVec = []\n",
    "    opensmile_f = []\n",
    "    opensmile_m = []\n",
    "    genderList = []\n",
    "\n",
    "    for index in range(name.shape[0]):\n",
    "         #add the opensmile list to correct list\n",
    "        if gender_dict[name[index]] == \"f\":\n",
    "            opensmile_f.append(feature[index][3])\n",
    "            genderList.append(gender_dict[name[index]])\n",
    "        elif gender_dict[name[index]] == \"m\":\n",
    "            opensmile_m.append(feature[index][3])\n",
    "            genderList.append(gender_dict[name[index]])\n",
    "        else:\n",
    "            print(\"Key error: name nonexistent!\")\n",
    "                \n",
    "        #create a num vec for ngram per record\n",
    "        ngramVec = np.zeros(ngramTemplate.shape[0])\n",
    "        for gram in ngramArr[index]:\n",
    "            try:\n",
    "                pos = np.argwhere(ngramTemplate==gram)\n",
    "                ngramVec[pos] += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        #create a pre-flattened list for each row of the record\n",
    "        #the dependent variable is in the first column of the first list\n",
    "        #tempList.form = [[list of all numeric features],[opensmile],[ngram],[embedding]]\n",
    "        tempList=[[feature[index][2]]]\n",
    "        for field in range(len(feature[0])):\n",
    "            #exclude the label (has been added to the first column)\n",
    "            if field != 2:\n",
    "                #add the ngram vector\n",
    "                if field == 4:\n",
    "                    tempList.append(ngramVec.tolist())\n",
    "                elif type(feature[index][field]) is list:\n",
    "                    tempList.append(feature[index][field])\n",
    "                else:\n",
    "                    tempList[0].append(feature[index][field])\n",
    "\n",
    "        #append the one-dimensional list to the preFeatureVec\n",
    "        preFeatureVec.append(np.array(tempList))\n",
    "\n",
    "    # zscore the acoustic and prosodic features by gender\n",
    "    opensmile_f = np.array(opensmile_f, dtype = float)\n",
    "    opensmile_m = np.array(opensmile_m, dtype = float)\n",
    "    f_mean = opensmile_f.mean(axis =0)\n",
    "    m_mean = opensmile_m.mean(axis =0)\n",
    "    f_std = opensmile_f.std(axis =0)\n",
    "    m_std = opensmile_m.std(axis =0)\n",
    " \n",
    "    for featureNum in range(len(preFeatureVec)):\n",
    "        #preFeatureVec[featureNum][1] is the opensmile list\n",
    "        if genderList[featureNum]==\"f\":\n",
    "            zscoredOpensmile = (np.array(preFeatureVec[featureNum][1], dtype=float)-f_mean)/f_std\n",
    "        else: \n",
    "            zscoredOpensmile = (np.array(preFeatureVec[featureNum][1], dtype=float)-m_mean)/m_std\n",
    "        zscoredOpensmile = zscoredOpensmile.tolist()\n",
    "    \n",
    "        #concatenate all lists in the preFeatureVec to become a whole single list\n",
    "        #simplifiedFeatureVec.form = [idName, speakerName, ], list of all numeric features,opensmile*6373, ngram, embedding*200]\n",
    "        processedFeatureVec.append([statementID[featureNum],name[featureNum]]+preFeatureVec[featureNum][0]+zscoredOpensmile+\\\n",
    "            preFeatureVec[featureNum][2] + preFeatureVec[featureNum][3])\n",
    "\n",
    "        print(2, len(preFeatureVec[featureNum][0]), len(zscoredOpensmile),len(preFeatureVec[featureNum][2]), len(preFeatureVec[featureNum][3]))\n",
    "\n",
    "    #insert the name info before the first column of the dataset\n",
    "    npFeatureArr = np.array(processedFeatureVec)\n",
    "    np.save(\"dataset.npy\",npFeatureArr)\n",
    "    return npFeatureArr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Indicate the subset type in the block below.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_feature_vectors(allFeatures, name, id, txtList, subsetType=\"regularOnly\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
