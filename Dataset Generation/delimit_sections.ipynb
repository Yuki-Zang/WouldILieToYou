{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `This notebook does automatic section detection of each video and generates a raw jsonl file per video for manual annotation. To run this file, make sure the following folders exist:`\n",
    "- sounds: this folder includes signals/buzzes that we use to delimit sections\n",
    "- audio: add all audios that needed to be processed here.\n",
    "- timeSeg: add all csv files (generated from diartization) of speaker turn info for videos to be processed here.\n",
    "- json: the folder to store the output files.\n",
    "\n",
    "-------------------------------------------------------\n",
    "- Intermediate file: `temp.wav`\n",
    "- Potential output files if `write_to_result()` is run: `segmentation_in_min.csv`, `segmentation_in_sec.csv`\n",
    "\n",
    "Last updated on August 3rd, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import numpy as np\n",
    "import librosa\n",
    "import scipy as sp\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "import moviepy.editor as mp\n",
    "import copy\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sound0.wav is the end-of-episode buzzle\n",
    "#sound1-4.wav is the result-revealing beap\n",
    "#sound5.wav is the speaker indicator sound in the quick truth-lie section\n",
    "\n",
    "def find_offset(within_file,find_file):\n",
    "    '''\n",
    "    parameters:\n",
    "    within_file: str. the file in which the target audio segement is to be found\n",
    "    find_file: str. the target file\n",
    "\n",
    "    return:\n",
    "    c: np.array (within_file.shape[0]-find_file.shape[0],) an array of correlation coefficient\n",
    "\n",
    "    reference:\n",
    "    https://dev.to/hiisi13/find-an-audio-within-another-audio-in-10-lines-of-python-1866\n",
    "    '''\n",
    "\n",
    "    y_within, sr_within = librosa.load(within_file, sr=16000)\n",
    "    y_find, _ = librosa.load(find_file, sr=sr_within)\n",
    "\n",
    "    c = sp.signal.correlate(y_within, y_find, mode='valid', method='fft')\n",
    "    # peak = np.argmax(c)\n",
    "    # print(f\"Offset: {offset}s\" )\n",
    "    # return c,round(peak / sr_within, 2)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateChunkTimes(within_file,beapNum,signalNum):\n",
    "    '''\n",
    "    parameters:\n",
    "    within_file: str. a wav file in which the target audio segement is to be found\n",
    "    beapNum: int, the number of total answer-revealing sounds to detect\n",
    "    signalNum: int, the number of speaker-designation-sounds to detect\n",
    "\n",
    "    return: \n",
    "    timeSegSecond: np.array, an array of time segments in seconds\n",
    "    timeSegMinute: np.array, an array of time segments in the form of \"dd:dd\"\n",
    "\n",
    "    NOTE: the matching templates are stored in the \"sounds\" folder\n",
    "    '''\n",
    "    \n",
    "    #find the beap sound\n",
    "    beapSeg = find_offset(within_file, \"../sounds/sound1.wav\")\n",
    "\n",
    "    #find in total five beaps in one video\n",
    "    indexList=[]\n",
    "    for i in range(beapNum):\n",
    "        maxIndex=np.argmax(beapSeg)\n",
    "        beapSeg[maxIndex-16000*120:maxIndex+16000*120,]=-1\n",
    "        indexList.append(maxIndex+5*16000)\n",
    "    indexList.sort()\n",
    "        \n",
    "    # #find the final beap\n",
    "    # #generate the temp.wav for the final buzzle\n",
    "    ffmpeg_extract_subclip(within_file, 1650, 1725, targetname=\"temp.wav\")\n",
    "    endSeg = find_offset(\"temp.wav\", \"../sounds/sound0.wav\")\n",
    "    endTime=np.argmax(endSeg)+1650*16000\n",
    "\n",
    "    #find the first silence\n",
    "    ffmpeg_extract_subclip(within_file, 75, 150, targetname=\"temp.wav\")\n",
    "    sound_file = AudioSegment.from_wav(\"temp.wav\")\n",
    "    audio_chunks = split_on_silence(sound_file, min_silence_len=850, silence_thresh=-40, keep_silence=100)\n",
    "    try:\n",
    "        startTime=round((len(audio_chunks[0])/1000+75)*16000)\n",
    "    except:\n",
    "        startTime=0\n",
    "\n",
    "    #find the start sound of the quick truth and lies\n",
    "    ffmpeg_extract_subclip(within_file, indexList[2]/16000, endTime/16000+1650, targetname=\"temp.wav\")\n",
    "    quickRoundSeg = find_offset(\"temp.wav\", \"../sounds/sound5.wav\")\n",
    "    indexList2=[]\n",
    "    for i in range(signalNum):\n",
    "        maxIndex=np.argmax(quickRoundSeg)\n",
    "        quickRoundSeg[maxIndex-16000*75:maxIndex+16000*75,]=-1\n",
    "        indexList2.append(maxIndex+indexList[2])\n",
    "    indexList2.sort()\n",
    "    \n",
    "    return clean_repeititions(indexList,indexList2,startTime,endTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_repeititions(beapList,signalList,startTime,endTime):\n",
    "    '''\n",
    "    parameters:\n",
    "    beapList: list of beaps (answer-revealing sounds)\n",
    "    signalList: list of signals (speaker-designation-sounds) in the quick truth-lie section\n",
    "    startTime: int of the start time\n",
    "    endTime: int of the end-of-episode buzzle\n",
    "    NOTE: all time in multiples of 16000 (sample_rate)\n",
    "\n",
    "    return:\n",
    "    timeSegSecond: np.array, an array of time segments in seconds\n",
    "    timeSegMinute: np.array, an array of time segments in the form of \"dd:dd\"\n",
    "    NOTE: for each video the length of the output array varies\n",
    "    '''\n",
    "    beapList_copy=copy.deepcopy(beapList)\n",
    "    signalList_copy=copy.deepcopy(signalList)\n",
    "\n",
    "    #delete some unnecesssary (too close to the endSeg) signal time\n",
    "    for item in signalList:\n",
    "        if endTime/16000+1650-item/16000<120:\n",
    "            signalList_copy.remove(item)\n",
    "\n",
    "    #delete some repetitive sounds (adjacent sounds for the result reveal and speaker indicator)\n",
    "    for beap in beapList:\n",
    "        for signal in signalList:\n",
    "            if abs(beap-signal)<60*16000:\n",
    "                beapList_copy.remove(beap)\n",
    "                break\n",
    "    \n",
    "    #finalize the timelist\n",
    "    timeSegSecond=[startTime]+beapList_copy+signalList_copy\n",
    "    timeSegSecond=np.sort(np.array(timeSegSecond))/16000\n",
    "    segminute=np.array(timeSegSecond)//60\n",
    "    segsec=np.round(np.array(timeSegSecond)%60)\n",
    "    timeSegMinute=np.array([\"{:02d}:{:02d}\".format(int(segminute[i]),int(segsec[i])) for i in range(segminute.shape[0])])\n",
    "    return timeSegSecond,timeSegMinute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_result_to_csv(outputFileNameSec=\"segmentation_in_sec.csv\",outputFileNameMin=\"segmentation_in_min.csv\",\\\n",
    "    subFolderName=\"wav\",beapNum=5,signalNum=3):\n",
    "    '''\n",
    "    parameter: \n",
    "    outputFileNameSec: str. the name of the csv that the info in seconds will be written into\n",
    "    outputFileNameMin: str. the name of the csv that the info in the form of \"dd:dd\" will be written into\n",
    "    subFolderName: str. the subdirectory where all audios (in wav) are stored and will be processed\n",
    "    beapNum: int, the number of beaps (answer-revealing sounds) to detect\n",
    "    signalNum: int, the number of signals (speaker-designation-sounds) to detect\n",
    "\n",
    "    output:\n",
    "    write the time segmentation info to the corrosponding csv files in the same folder\n",
    "\n",
    "    NOTE: the beapNum and signalNum might not be the final number as time segmenets close to each other\n",
    "    (i.e. indicating the same interval) will not both be recorded\n",
    "    '''\n",
    "    dataSec=[]\n",
    "    dataMin=[]\n",
    "\n",
    "    file_names=os.listdir(subFolderName)\n",
    "    try:\n",
    "        file_names.remove(\".DS_Store\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for file in file_names:\n",
    "        second,minute=generateChunkTimes(subFolderName+\"/\"+file,beapNum,signalNum)\n",
    "        dataSec.append(second)\n",
    "        dataMin.append(minute)\n",
    "\n",
    "    dfs=pd.DataFrame(dataSec)\n",
    "    dfm=pd.DataFrame(dataMin)\n",
    "    dfs.insert(0,\"filename\",file_names)\n",
    "    dfm.insert(0,\"filename\",file_names)\n",
    "\n",
    "    dfs.to_csv(outputFileNameSec)\n",
    "    dfm.to_csv(outputFileNameMin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_json_file_per_video(subFolderName=\"../audio\",beapNum=5,signalNum=3):\n",
    "    '''\n",
    "    parameter:\n",
    "    subFolderName: str. the subdirectory where all audios (in wav) are stored and will be processed\n",
    "    beapNum: int, the number of beaps (answer-revealing sounds) to detect\n",
    "    signalNum: int, the number of signals (speaker-designation-sounds) to detect\n",
    "\n",
    "    take in the time segment files from the timeSeg folder,\n",
    "    takes in the timeSeg, and store all time into a json file in the folder \"json\"\n",
    "    '''\n",
    "\n",
    "    file_names=os.listdir(subFolderName)\n",
    "    try:\n",
    "        file_names.remove(\".DS_Store\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for file in file_names:\n",
    "        print(file)\n",
    "        audio_spans=[]\n",
    "        second,minute=generateChunkTimes(subFolderName+\"/\"+file,beapNum,signalNum)\n",
    "        df=pd.read_csv(\"timeSeg/\"+file[:-4]+\"time.csv\",usecols=[1,2])\n",
    "\n",
    "        #the mark to indicate where to start\n",
    "        start=0\n",
    "        positionMark=0\n",
    "\n",
    "        #adds SPEAKER time segmentation\n",
    "        for sec in second:\n",
    "            for i in range(start,len(df)):\n",
    "                if sec<df[\"endAt\"][i]:\n",
    "                    speakerStart=df[\"endAt\"][i]\n",
    "                    try:\n",
    "                        speakerEnd=df[\"endAt\"][i+2]\n",
    "                    except:\n",
    "                        try: \n",
    "                            speakerEnd=df[\"endAt\"][i+1]\n",
    "                        except:\n",
    "                            speakerEnd=df[\"endAt\"][i]\n",
    "                            \n",
    "                    audio_spans.append({\n",
    "                        \"start\": speakerStart,\n",
    "                        \"end\":speakerEnd,\n",
    "                        \"label\": \"SPEAKER\",\n",
    "                    })\n",
    "                    start=positionMark\n",
    "                    break\n",
    "                positionMark+=1\n",
    "\n",
    "        #add SEGMENT_TRUTH time segmentation\n",
    "        for i in range(len(second)-1):\n",
    "            audio_spans.append({\n",
    "                \"start\": second[i],\n",
    "                \"end\":second[i+1],\n",
    "                \"label\": \"SEGMENT_TRUTH\",\n",
    "            })\n",
    "\n",
    "        #make the big dictionary and write into json file\n",
    "        jsonDic={\n",
    "            \"video\":\"videoAnnotate\\\\\"+file[:-3]+\"mp4\",\n",
    "            \"path\":\"videoAnnotate\\\\\"+file[:-3]+\"mp4\",\n",
    "            \"audio_spans\":audio_spans,\n",
    "        }\n",
    "\n",
    "        with open(\"json/\"+file[:-3]+\"jsonl\", \"w\") as outfile:\n",
    "            json.dump(jsonDic, outfile)\n",
    "            outfile.write(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_result_to_csv(outputFileNameSec=\"timeSegSec01.csv\",outputFileNameMin=\"timeSegMin01.csv\", beapNum=6)\n",
    "# write_result_to_csv()\n",
    "generate_json_file_per_video(beapNum=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wilty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
